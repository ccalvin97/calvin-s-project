{"cells":[{"metadata":{},"cell_type":"markdown","source":"Keras Embedding CRF + FC"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nprint(os.listdir(\"../input\"))\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras","execution_count":223,"outputs":[{"output_type":"stream","text":"['ner.csv', 'ner_dataset.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/ner_dataset.csv\", encoding=\"latin1\")","execution_count":224,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.fillna(method=\"ffill\")\nwords = list(set(data[\"Word\"].values))\nn_words = len(words)\ntags = list(set(data[\"Tag\"].values))\nn_tags = len(tags)","execution_count":225,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SentenceGetter(object):\n    \"\"\"Class to Get the sentence in this format:\n    [(Token_1, Part_of_Speech_1, Tag_1), ..., (Token_n, Part_of_Speech_1, Tag_1)]\"\"\"\n    def __init__(self, data):\n        \"\"\"Args:\n            data is the pandas.DataFrame which contains the above dataset\"\"\"\n        self.n_sent = 1\n        self.data = data\n        self.empty = False\n        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n                                                           s[\"POS\"].values.tolist(),\n                                                           s[\"Tag\"].values.tolist())]\n        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n    \n    def get_next(self):\n        \"\"\"Return one sentence\"\"\"\n        try:\n            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n            self.n_sent += 1\n            return s\n        except:\n            return None\n        \ngetter = SentenceGetter(data)\nsent = getter.get_next()\n# Get all the sentences\nsentences = getter.sentences","execution_count":226,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2idx = {w: i + 2 for i, w in enumerate(words)}\nword2idx[\"UNK\"] = 1 \nword2idx[\"PAD\"] = 0 \n\n# Vocabulary Key:token_index -> Value:word\nidx2word = {i: w for w, i in word2idx.items()}\n\n# Vocabulary Key:Label/Tag -> Value:tag_index\n# The first entry is reserved for PAD\ntag2idx = {t: i+1 for i, t in enumerate(tags)}\ntag2idx[\"PAD\"] = 0\n\n# Vocabulary Key:tag_index -> Value:Label/Tag\nidx2tag = {i: w for w, i in tag2idx.items()}\n\nfrom keras.preprocessing.sequence import pad_sequences\n# Convert each sentence from list of Token to list of word_index\nX = [[word2idx[w[0]] for w in s] for s in sentences]\nX = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=word2idx[\"PAD\"])\n\n# Convert Tag/Label to tag_index\ny = [[tag2idx[w[2]] for w in s] for s in sentences]\ny = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"PAD\"]) \n\nfrom keras.utils import to_categorical # One-Hot encode\ny = [to_categorical(i, num_classes=n_tags+1) for i in y]  # n_tags+1(PAD)\n\nfrom sklearn.model_selection import train_test_split\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)","execution_count":227,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32 ################# batch size\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom keras import backend as K\nfrom keras.models import Model, Input\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\nfrom keras_contrib.layers import CRF\nsess = tf.Session()\nK.set_session(sess)","execution_count":228,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NN Start"},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install git+https://www.github.com/keras-team/keras-contrib.git","execution_count":229,"outputs":[{"output_type":"stream","text":"Collecting git+https://www.github.com/keras-team/keras-contrib.git\n  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-sh2t50qi\n  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-sh2t50qi\nRequirement already satisfied (use --upgrade to upgrade): keras-contrib==2.0.8 from git+https://www.github.com/keras-team/keras-contrib.git in /opt/conda/lib/python3.6/site-packages\nRequirement already satisfied: keras in /opt/conda/lib/python3.6/site-packages (from keras-contrib==2.0.8) (2.2.4)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8) (1.1.0)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8) (2.9.0)\nRequirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8) (1.0.8)\nRequirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8) (1.2.1)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8) (1.17.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8) (5.1.1)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8) (1.12.0)\nBuilding wheels for collected packages: keras-contrib\n  Building wheel for keras-contrib (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101066 sha256=ef49863d8be64afd96e68a3affc392981befee77dd1cf28a36bd9d1ef71eb5f1\n  Stored in directory: /tmp/pip-ephem-wheel-cache-uevju8wq/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\nSuccessfully built keras-contrib\nNote: you may need to restart the kernel to use updated packages.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model, Input\nfrom keras.layers.merge import add\nfrom keras import initializers, regularizers, constraints\nfrom livelossplot.keras import PlotLossesCallback\nfrom keras.layers.normalization import BatchNormalization\nfrom keras_contrib.layers import CRF","execution_count":230,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 512\nepochs = 20\nmax_len = 80  \nEMBEDDING = 40  \n\ninput_text = Input(shape=(max_len,))\nmodel = Embedding(input_dim=n_words+2, output_dim=EMBEDDING, # n_words + 2 (PAD & UNK)\n                  input_length=max_len, mask_zero=True)(input_text)  \nmodel = Bidirectional(LSTM(units=80, return_sequences=True,\n                           recurrent_dropout=0.1))(model)  \nmodel = Bidirectional(LSTM(units=50, return_sequences=True,\n                           recurrent_dropout=0.1))(model)  \nmodel = TimeDistributed(Dense(50, activation=\"relu\"))(model)  \ncrf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\nout = crf(model)\nmodel = Model(input_text, out)\nmodel.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=[crf.accuracy])\nmodel.summary()","execution_count":242,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n  warnings.warn('CRF.loss_function is deprecated '\n/opt/conda/lib/python3.6/site-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n  warnings.warn('CRF.accuracy is deprecated and it '\n","name":"stderr"},{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_32 (InputLayer)        (None, 80)                0         \n_________________________________________________________________\nembedding_14 (Embedding)     (None, 80, 40)            1407200   \n_________________________________________________________________\nbidirectional_94 (Bidirectio (None, 80, 160)           77440     \n_________________________________________________________________\nbidirectional_95 (Bidirectio (None, 80, 100)           84400     \n_________________________________________________________________\ntime_distributed_21 (TimeDis (None, 80, 50)            5050      \n_________________________________________________________________\ncrf_24 (CRF)                 (None, 80, 18)            1278      \n=================================================================\nTotal params: 1,575,368\nTrainable params: 1,575,368\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_tr, np.array(y_tr), batch_size=batch_size, epochs=epochs,validation_split=0.1, verbose=2)","execution_count":243,"outputs":[{"output_type":"stream","text":"Train on 38846 samples, validate on 4317 samples\nEpoch 1/20\n - 97s - loss: 10.7374 - crf_viterbi_accuracy: 0.7440 - val_loss: 10.3918 - val_crf_viterbi_accuracy: 0.8424\nEpoch 2/20\n - 61s - loss: 10.2273 - crf_viterbi_accuracy: 0.8474 - val_loss: 10.2389 - val_crf_viterbi_accuracy: 0.8417\nEpoch 3/20\n - 61s - loss: 9.9402 - crf_viterbi_accuracy: 0.8756 - val_loss: 9.9433 - val_crf_viterbi_accuracy: 0.9066\nEpoch 4/20\n - 61s - loss: 9.7742 - crf_viterbi_accuracy: 0.9265 - val_loss: 9.8567 - val_crf_viterbi_accuracy: 0.9315\nEpoch 5/20\n - 60s - loss: 9.7133 - crf_viterbi_accuracy: 0.9440 - val_loss: 9.8286 - val_crf_viterbi_accuracy: 0.9387\nEpoch 6/20\n - 60s - loss: 9.6844 - crf_viterbi_accuracy: 0.9526 - val_loss: 9.8139 - val_crf_viterbi_accuracy: 0.9435\nEpoch 7/20\n - 60s - loss: 9.6643 - crf_viterbi_accuracy: 0.9596 - val_loss: 9.7989 - val_crf_viterbi_accuracy: 0.9486\nEpoch 8/20\n - 60s - loss: 9.6476 - crf_viterbi_accuracy: 0.9647 - val_loss: 9.7798 - val_crf_viterbi_accuracy: 0.9537\nEpoch 9/20\n - 61s - loss: 9.6318 - crf_viterbi_accuracy: 0.9688 - val_loss: 9.7651 - val_crf_viterbi_accuracy: 0.9586\nEpoch 10/20\n - 60s - loss: 9.6202 - crf_viterbi_accuracy: 0.9716 - val_loss: 9.7545 - val_crf_viterbi_accuracy: 0.9607\nEpoch 11/20\n - 60s - loss: 9.6115 - crf_viterbi_accuracy: 0.9736 - val_loss: 9.7505 - val_crf_viterbi_accuracy: 0.9617\nEpoch 12/20\n - 60s - loss: 9.6050 - crf_viterbi_accuracy: 0.9752 - val_loss: 9.7466 - val_crf_viterbi_accuracy: 0.9625\nEpoch 13/20\n - 60s - loss: 9.5993 - crf_viterbi_accuracy: 0.9767 - val_loss: 9.7445 - val_crf_viterbi_accuracy: 0.9633\nEpoch 14/20\n - 60s - loss: 9.5951 - crf_viterbi_accuracy: 0.9776 - val_loss: 9.7438 - val_crf_viterbi_accuracy: 0.9634\nEpoch 15/20\n - 60s - loss: 9.5911 - crf_viterbi_accuracy: 0.9790 - val_loss: 9.7439 - val_crf_viterbi_accuracy: 0.9628\nEpoch 16/20\n - 60s - loss: 9.5879 - crf_viterbi_accuracy: 0.9798 - val_loss: 9.7436 - val_crf_viterbi_accuracy: 0.9636\nEpoch 17/20\n - 60s - loss: 9.5846 - crf_viterbi_accuracy: 0.9809 - val_loss: 9.7441 - val_crf_viterbi_accuracy: 0.9636\nEpoch 18/20\n - 60s - loss: 9.5820 - crf_viterbi_accuracy: 0.9815 - val_loss: 9.7469 - val_crf_viterbi_accuracy: 0.9620\nEpoch 19/20\n - 61s - loss: 9.5792 - crf_viterbi_accuracy: 0.9825 - val_loss: 9.7468 - val_crf_viterbi_accuracy: 0.9636\nEpoch 20/20\n - 60s - loss: 9.5767 - crf_viterbi_accuracy: 0.9832 - val_loss: 9.7500 - val_crf_viterbi_accuracy: 0.9617\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_cat = model.predict(X_te)\npred = np.argmax(pred_cat, axis=-1)\ny_te_true = np.argmax(y_te, -1)\nfrom sklearn_crfsuite.metrics import flat_classification_report\n# Convert the index to tag\npred_tag = [[idx2tag[i] for i in row] for row in pred]\ny_te_true_tag = [[idx2tag[i] for i in row] for row in y_te_true] \n\nreport = flat_classification_report(y_pred=pred_tag, y_true=y_te_true_tag)\nprint(report)","execution_count":247,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n       B-art       0.00      0.00      0.00        33\n       B-eve       0.00      0.00      0.00        32\n       B-geo       0.82      0.90      0.86      3690\n       B-gpe       0.96      0.93      0.95      1567\n       B-nat       0.00      0.00      0.00        18\n       B-org       0.70      0.70      0.70      2111\n       B-per       0.85      0.76      0.80      1638\n       B-tim       0.87      0.88      0.88      2056\n       I-art       0.00      0.00      0.00        31\n       I-eve       0.50      0.03      0.06        33\n       I-geo       0.73      0.79      0.76       719\n       I-gpe       0.80      0.33      0.47        12\n       I-nat       0.00      0.00      0.00         2\n       I-org       0.71      0.77      0.74      1800\n       I-per       0.87      0.78      0.82      1698\n       I-tim       0.74      0.73      0.73       699\n           O       0.99      0.99      0.99     88113\n         PAD       1.00      1.00      1.00    279428\n\n    accuracy                           0.99    383680\n   macro avg       0.59      0.53      0.54    383680\nweighted avg       0.99      0.99      0.99    383680\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}